# ğŸ¤– Sistema RAG HÃ­brido con vLLM

Un sistema avanzado de Retrieval-Augmented Generation (RAG) que combina bÃºsqueda vectorial y textual para proporcionar respuestas precisas basadas en documentos. Especializado para consultas tÃ©cnicas sobre redes WiFi y documentaciÃ³n empresarial.

## ğŸš€ CaracterÃ­sticas Principales

### âœ¨ RAG HÃ­brido Avanzado
- **BÃºsqueda Vectorial**: Usando PostgreSQL con pgvector para similitud semÃ¡ntica
- **BÃºsqueda Textual**: Ãndice Lunr para coincidencias exactas de tÃ©rminos
- **FusiÃ³n RRF**: Reciprocal Rank Fusion para combinar resultados optimalmente
- **Re-ranking**: CrossEncoder para mejorar la relevancia final

### ğŸ”§ TecnologÃ­as Core
- **vLLM**: GeneraciÃ³n de texto y embeddings de alta performance
- **PostgreSQL + pgvector**: Base de datos vectorial escalable
- **Streamlit**: Interfaz web interactiva
- **Marker**: Procesamiento avanzado de documentos DOCX
- **Transformers**: Modelos de embedding y tokenizaciÃ³n

### ğŸ“„ Procesamiento de Documentos
- Soporte nativo para archivos DOCX
- ExtracciÃ³n inteligente de texto y tablas
- Chunking adaptativo con solapamiento
- PreservaciÃ³n de estructura de documentos

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documentos    â”‚    â”‚   Procesamiento â”‚    â”‚   Almacenamientoâ”‚
â”‚     DOCX        â”‚â”€â”€â”€â–¶â”‚     Marker      â”‚â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   + pgvector    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   Interfaz      â”‚    â”‚   RAG HÃ­brido   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Streamlit     â”‚â—€â”€â”€â”€â”‚   Orchestrator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      vLLM       â”‚
                       â”‚   Embeddings    â”‚
                       â”‚  + GeneraciÃ³n   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de BÃºsqueda HÃ­brida

1. **Query del Usuario** â†’ Procesamiento en paralelo
2. **BÃºsqueda Vectorial** â†’ Embeddings + Similitud coseno
3. **BÃºsqueda Textual** â†’ Ãndice Lunr + TF-IDF
4. **FusiÃ³n RRF** â†’ Combina rankings con pesos optimizados
5. **Re-ranking** â†’ CrossEncoder para refinamiento final
6. **GeneraciÃ³n** â†’ vLLM produce respuesta contextualizada

## ğŸ“‹ Prerrequisitos

- **Python 3.8+**
- **PostgreSQL 12+** con extensiÃ³n pgvector
- **vLLM Server** configurado y ejecutÃ¡ndose
- **8GB+ RAM** recomendado para modelos de embedding

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el Repositorio
```bash
git clone <tu-repositorio>
cd rag_vllm
```

### 2. Crear Entorno Virtual
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

### 3. Instalar Dependencias

#### OpciÃ³n A: InstalaciÃ³n Completa (Recomendado)
```bash
pip install -r requirements.txt
```

#### OpciÃ³n B: InstalaciÃ³n MÃ­nima (Sin Marker)
Si prefieres una instalaciÃ³n mÃ¡s ligera sin `marker-pdf`:
```bash
pip install -r requirements-minimal.txt
```

> **Nota**: Con la instalaciÃ³n mÃ­nima solo podrÃ¡s usar `python-docx` para procesar documentos DOCX. Marker proporciona mejor calidad de extracciÃ³n pero requiere mÃ¡s dependencias.



### 4. Configurar Variables de Entorno

Crear archivo `.env` en la raÃ­z del proyecto:

```env
# ConfiguraciÃ³n vLLM
VLLM_ENDPOINT=http://192.168...
VLLM_EMBED=http://192.168...
VLLM_MODEL_GENERATION=tu-modelo-generativo
EMBEDDING_API_ENDPOINT=http://192.168...

# Base de datos
DB_CONNECTION_STRING=postgresql://usuario:contraseÃ±a@localhost/rag_db
```

### 5. Crear Estructura de Directorios
```bash
mkdir -p data/docx
```

## ğŸ“š Uso

### 1. Preparar Documentos
Coloca tus archivos DOCX en la carpeta `data/docx/`:
```
data/
â””â”€â”€ docx/
    â”œâ”€â”€ documento1.docx
    â”œâ”€â”€ documento2.docx
    â””â”€â”€ ...
```

### 2. Procesar e Ingestar Documentos

#### OpciÃ³n A: Con Marker (Recomendado)
```bash
python scripts/ingest_data_marker.py
```

#### OpciÃ³n B: Con python-docx (Alternativa)
```bash
python scripts/ingest_data.py
```


### 3. Ejecutar la AplicaciÃ³n
```bash
streamlit run app/ui_main.py
```

### 4. Interactuar con el Sistema
1. Abre tu navegador en `http://localhost:8501`
2. Escribe tu pregunta sobre el contenido de los documentos
3. El sistema realizarÃ¡ bÃºsqueda hÃ­brida y generarÃ¡ una respuesta contextualizada

## ğŸ”§ ConfiguraciÃ³n Avanzada

### ParÃ¡metros de Chunking
En `scripts/ingest_data_marker.py`:
```python
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer=tokenizer, 
    chunk_size=768,      # TamaÃ±o del chunk
    chunk_overlap=75     # Solapamiento entre chunks
)
```

### ConfiguraciÃ³n RAG HÃ­brido
En `app/rag_logic.py`:
```python
# Factor para bÃºsquedas iniciales
initial_retrieval_factor = 2

# LÃ­mite de documentos para contexto
search_limit = limit * initial_retrieval_factor

# ParÃ¡metro k para RRF
k_val = 60
```

### Modelos de Embedding
Modelos compatibles:
- `sentence-transformers/all-MiniLM-L6-v2` (384 dim)
- `nomic-ai/nomic-embed-text-v1.5` (768 dim)
- Modelos personalizados vÃ­a vLLM

## ğŸ“ Estructura del Proyecto

```
rag_vllm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ui_main.py              # Interfaz Streamlit
â”‚   â”œâ”€â”€ rag_logic.py            # LÃ³gica RAG hÃ­brida
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ system_prompt.txt   # Prompt del sistema
â”‚       â””â”€â”€ wifi_expert_prompt.txt # Prompt especializado
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest_data.py          # Ingesta con python-docx
â”‚   â””â”€â”€ ingest_data_marker.py   # Ingesta con Marker
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docx/                   # Documentos DOCX
â”œâ”€â”€ venv/                       # Entorno virtual
â”œâ”€â”€ requirements.txt            # Dependencias completas
â”œâ”€â”€ requirements-minimal.txt    # Dependencias sin marker-pdf
â”œâ”€â”€ .env                        # Variables de entorno
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### Caching
- Streamlit cachea automÃ¡ticamente:
  - Documentos de la BD
  - Ãndices Lunr
  - Modelos CrossEncoder
  - Cliente vLLM


#### InstalaciÃ³n Alternativa
Si tienes problemas con `requirements.txt`:
```bash
# Instalar paso a paso las dependencias core
pip install streamlit numpy requests python-dotenv
pip install transformers sentence-transformers
pip install psycopg2-binary pgvector
pip install lunr python-docx
pip install openai
```
