# ğŸ¤– Sistema RAG HÃ­brido con vLLM

Un sistema avanzado de Retrieval-Augmented Generation (RAG) que combina bÃºsqueda vectorial y textual para proporcionar respuestas precisas basadas en documentos. Especializado para consultas tÃ©cnicas sobre redes WiFi y documentaciÃ³n empresarial.

## ğŸš€ CaracterÃ­sticas Principales

### âœ¨ RAG HÃ­brido Avanzado
- **BÃºsqueda Vectorial**: Usando PostgreSQL con pgvector para similitud semÃ¡ntica
- **BÃºsqueda Textual**: Ãndice Lunr para coincidencias exactas de tÃ©rminos
- **FusiÃ³n RRF**: Reciprocal Rank Fusion para combinar resultados optimalmente
- **Re-ranking**: CrossEncoder para mejorar la relevancia final

### ğŸ”§ TecnologÃ­as Core
- **vLLM**: GeneraciÃ³n de texto y embeddings de alta performance
- **PostgreSQL + pgvector**: Base de datos vectorial escalable
- **Streamlit**: Interfaz web interactiva
- **Marker**: Procesamiento avanzado de documentos DOCX
- **Transformers**: Modelos de embedding y tokenizaciÃ³n

### ğŸ“„ Procesamiento de Documentos
- Soporte nativo para archivos DOCX
- ExtracciÃ³n inteligente de texto y tablas
- Chunking adaptativo con solapamiento
- PreservaciÃ³n de estructura de documentos

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documentos    â”‚    â”‚   Procesamiento â”‚    â”‚   Almacenamientoâ”‚
â”‚     DOCX        â”‚â”€â”€â”€â–¶â”‚     Marker      â”‚â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   + pgvector    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   Interfaz      â”‚    â”‚   RAG HÃ­brido   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Streamlit     â”‚â—€â”€â”€â”€â”‚   Orchestrator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      vLLM       â”‚
                       â”‚   Embeddings    â”‚
                       â”‚  + GeneraciÃ³n   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de BÃºsqueda HÃ­brida

1. **Query del Usuario** â†’ Procesamiento en paralelo
2. **BÃºsqueda Vectorial** â†’ Embeddings + Similitud coseno
3. **BÃºsqueda Textual** â†’ Ãndice Lunr + TF-IDF
4. **FusiÃ³n RRF** â†’ Combina rankings con pesos optimizados
5. **Re-ranking** â†’ CrossEncoder para refinamiento final
6. **GeneraciÃ³n** â†’ vLLM produce respuesta contextualizada

## ğŸ“‹ Prerrequisitos

- **Python 3.8+**
- **PostgreSQL 12+** con extensiÃ³n pgvector
- **vLLM Server** configurado y ejecutÃ¡ndose
- **8GB+ RAM** recomendado para modelos de embedding

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el Repositorio
```bash
git clone <tu-repositorio>
cd rag_vllm
```

### 2. Crear Entorno Virtual
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

### 3. Instalar Dependencias

#### OpciÃ³n A: InstalaciÃ³n Completa (Recomendado)
```bash
pip install -r requirements.txt
```

#### OpciÃ³n B: InstalaciÃ³n MÃ­nima (Sin Marker)
Si prefieres una instalaciÃ³n mÃ¡s ligera sin `marker-pdf`:
```bash
pip install -r requirements-minimal.txt
```

> **Nota**: Con la instalaciÃ³n mÃ­nima solo podrÃ¡s usar `python-docx` para procesar documentos DOCX. Marker proporciona mejor calidad de extracciÃ³n pero requiere mÃ¡s dependencias.

### 4. Configurar PostgreSQL con pgvector
```sql
-- Crear base de datos
CREATE DATABASE rag_db;

-- Conectar a la base de datos
\c rag_db;

-- Instalar extensiÃ³n pgvector
CREATE EXTENSION vector;

-- Verificar instalaciÃ³n
SELECT * FROM pg_extension WHERE extname = 'vector';
```

### 5. Configurar Variables de Entorno

Crear archivo `.env` en la raÃ­z del proyecto:

```env
# ConfiguraciÃ³n vLLM
VLLM_ENDPOINT=http://192.168.1.10:8002
VLLM_EMBED=http://192.168.1.10:8003/embed
VLLM_MODEL_GENERATION=tu-modelo-generativo
EMBEDDING_API_ENDPOINT=http://192.168.1.10:8003/embed

# Base de datos
DB_CONNECTION_STRING=postgresql://usuario:contraseÃ±a@localhost/rag_db
```

### 6. Crear Estructura de Directorios
```bash
mkdir -p data/docx
```

## ğŸ“š Uso

### 1. Preparar Documentos
Coloca tus archivos DOCX en la carpeta `data/docx/`:
```
data/
â””â”€â”€ docx/
    â”œâ”€â”€ documento1.docx
    â”œâ”€â”€ documento2.docx
    â””â”€â”€ ...
```

### 2. Procesar e Ingestar Documentos

#### OpciÃ³n A: Con Marker (Recomendado)
```bash
python scripts/ingest_data_marker.py
```

#### OpciÃ³n B: Con python-docx (Alternativa)
```bash
python scripts/ingest_data.py
```

**Salida esperada:**
```
Iniciando script de ingesta para archivos DOCX usando Marker...
Marker importado correctamente.
ConexiÃ³n a PostgreSQL establecida.
Tabla 'documents_wifi' verificada/creada.

Procesando archivo: data/docx/Expert_Wi-Fi_limpio.docx...
  Intentando extracciÃ³n con Marker...
  Total de 45 fragmentos generados para 'Expert_Wi-Fi_limpio.docx'.
  Archivo 'Expert_Wi-Fi_limpio.docx' procesado: 45 fragmentos insertados/actualizados.

Proceso de ingestiÃ³n a PostgreSQL completado.
```

### 3. Ejecutar la AplicaciÃ³n
```bash
streamlit run app/ui_main.py
```

### 4. Interactuar con el Sistema
1. Abre tu navegador en `http://localhost:8501`
2. Escribe tu pregunta sobre el contenido de los documentos
3. El sistema realizarÃ¡ bÃºsqueda hÃ­brida y generarÃ¡ una respuesta contextualizada

## ğŸ”§ ConfiguraciÃ³n Avanzada

### ParÃ¡metros de Chunking
En `scripts/ingest_data_marker.py`:
```python
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer=tokenizer, 
    chunk_size=768,      # TamaÃ±o del chunk
    chunk_overlap=75     # Solapamiento entre chunks
)
```

### ConfiguraciÃ³n RAG HÃ­brido
En `app/rag_logic.py`:
```python
# Factor para bÃºsquedas iniciales
initial_retrieval_factor = 2

# LÃ­mite de documentos para contexto
search_limit = limit * initial_retrieval_factor

# ParÃ¡metro k para RRF
k_val = 60
```

### Modelos de Embedding
Modelos compatibles:
- `sentence-transformers/all-MiniLM-L6-v2` (384 dim)
- `nomic-ai/nomic-embed-text-v1.5` (768 dim)
- Modelos personalizados vÃ­a vLLM

## ğŸ“ Estructura del Proyecto

```
rag_vllm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ui_main.py              # Interfaz Streamlit
â”‚   â”œâ”€â”€ rag_logic.py            # LÃ³gica RAG hÃ­brida
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ system_prompt.txt   # Prompt del sistema
â”‚       â””â”€â”€ wifi_expert_prompt.txt # Prompt especializado
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest_data.py          # Ingesta con python-docx
â”‚   â””â”€â”€ ingest_data_marker.py   # Ingesta con Marker
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docx/                   # Documentos DOCX
â”œâ”€â”€ venv/                       # Entorno virtual
â”œâ”€â”€ requirements.txt            # Dependencias completas
â”œâ”€â”€ requirements-minimal.txt    # Dependencias sin marker-pdf
â”œâ”€â”€ .env                        # Variables de entorno
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ¯ Casos de Uso

### Consultas TÃ©cnicas WiFi
- "Â¿CuÃ¡les son los estÃ¡ndares de seguridad WiFi recomendados?"
- "Â¿CÃ³mo configurar una red mesh empresarial?"
- "Â¿QuÃ© protocolos de autenticaciÃ³n se deben usar?"

### DocumentaciÃ³n Empresarial
- PolÃ­ticas de seguridad
- Procedimientos tÃ©cnicos
- Manuales de configuraciÃ³n
- GuÃ­as de troubleshooting

## âš¡ OptimizaciÃ³n de Performance

### Base de Datos
```sql
-- Crear Ã­ndice vectorial para bÃºsquedas mÃ¡s rÃ¡pidas
CREATE INDEX ON documents_wifi USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Actualizar estadÃ­sticas
ANALYZE documents_wifi;
```

### Memoria y CPU
- **RAM**: 8GB+ para modelos de embedding
- **CPU**: 4+ cores para procesamiento paralelo
- **GPU**: Opcional, mejora performance de vLLM

### Caching
- Streamlit cachea automÃ¡ticamente:
  - Documentos de la BD
  - Ãndices Lunr
  - Modelos CrossEncoder
  - Cliente vLLM

## ğŸ” Troubleshooting

### Error de ConexiÃ³n vLLM
```
Error: No se pudieron cargar los recursos necesarios
```
**SoluciÃ³n**: Verificar que vLLM estÃ© ejecutÃ¡ndose:
```bash
curl http://192.168.1.10:8002/v1/models
```

### Error PostgreSQL
```
Error conectando a PostgreSQL
```
**SoluciÃ³n**: 
1. Verificar que PostgreSQL estÃ© corriendo
2. Instalar pgvector: `CREATE EXTENSION vector;`
3. Revisar string de conexiÃ³n en `.env`

### Problemas de InstalaciÃ³n

#### Marker Installation Issues
```python
# Si Marker falla, el sistema usa python-docx automÃ¡ticamente
try:
    from marker import Marker
    marker_available = True
except ImportError:
    marker_available = False
    # Fallback a python-docx
```

#### Dependencias PyTorch/CUDA
Si tienes problemas con PyTorch en GPU:
```bash
# CPU only (mÃ¡s ligero)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# CUDA 11.8 (si tienes GPU NVIDIA)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

#### PostgreSQL psycopg2 Issues
En algunos sistemas, `psycopg2-binary` puede fallar:
```bash
# Ubuntu/Debian
sudo apt-get install postgresql-dev python3-dev

# Windows (usar conda)
conda install psycopg2

# macOS
brew install postgresql
```

#### InstalaciÃ³n Alternativa
Si tienes problemas con `requirements.txt`:
```bash
# Instalar paso a paso las dependencias core
pip install streamlit numpy requests python-dotenv
pip install transformers sentence-transformers
pip install psycopg2-binary pgvector
pip install lunr python-docx
pip install openai
```

### Performance Lenta
1. **Reducir chunk_size**: Menos contexto, respuestas mÃ¡s rÃ¡pidas
2. **Ajustar lÃ­mites**: Reducir `NUM_DOCS_FOR_CONTEXT`
3. **Ãndices BD**: Crear Ã­ndices vectoriales apropiados

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ‘¥ Autores

- Tu Nombre - *Desarrollo inicial* - [TuGitHub](https://github.com/tu-usuario)

## ğŸ™ Agradecimientos

- **vLLM Team** - Sistema de inferencia eficiente
- **pgvector** - ExtensiÃ³n vectorial para PostgreSQL
- **Marker** - Procesamiento avanzado de documentos
- **Streamlit** - Framework de aplicaciones web
- **Sentence Transformers** - Modelos de embedding pre-entrenados 